{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "title-cell",
   "metadata": {},
   "source": [
    "# üè• MedMCQA Structured-JSON Classifier\n",
    "## End-to-End Healthcare ML Pipeline: SFT ‚Üí GRPO ‚Üí Evaluation\n",
    "\n",
    "---\n",
    "\n",
    "> **Goal**: Fine-tune a tiny open-source LLM (`Qwen3-0.6B`) on real medical exam questions,\n",
    "> teaching it to output structured JSON predictions ‚Äî then apply reinforcement learning\n",
    "> (GRPO) to sharpen its answers, and rigorously evaluate all three model checkpoints.\n",
    "\n",
    "| | |\n",
    "|---|---|\n",
    "| **Model** | `Qwen/Qwen3-0.6B` (0.6B params, Apache-2.0) |\n",
    "| **Dataset** | `openlifescienceai/medmcqa` ‚Äî AIIMS & NEET PG medical entrance exams |\n",
    "| **Infrastructure** | Hugging Face Jobs ‚Äî `t4-small` (NVIDIA T4 16GB) |\n",
    "| **Total cost** | ~\\$0.90 |\n",
    "| **Total time** | ~70 min (sequential) |\n",
    "| **SFT Model** | [wei25/qwen3-0.6b-medmcqa-sft](https://huggingface.co/wei25/qwen3-0.6b-medmcqa-sft) |\n",
    "| **GRPO Model** | [wei25/qwen3-0.6b-medmcqa-grpo](https://huggingface.co/wei25/qwen3-0.6b-medmcqa-grpo) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "task-overview",
   "metadata": {},
   "source": [
    "## 1. The Task: Medical Multiple-Choice Question Classification\n",
    "\n",
    "### What is MedMCQA?\n",
    "\n",
    "[MedMCQA](https://huggingface.co/datasets/openlifescienceai/medmcqa) is a large-scale dataset of **194,000+ multiple-choice questions** from AIIMS and NEET PG (Indian medical entrance exams), covering subjects like Anatomy, Pharmacology, Pathology, and more.\n",
    "\n",
    "Each question has:\n",
    "- A **question** stem\n",
    "- **4 options**: `opa`, `opb`, `opc`, `opd`\n",
    "- A **correct option** (`cop`): integer 0‚Äì3 mapping to A‚ÄìD\n",
    "- An **explanation** (`exp`): clinical reasoning\n",
    "\n",
    "### The Structured Output Format\n",
    "\n",
    "Rather than free-text answers, we teach the model to output **strict JSON** ‚Äî making it machine-parseable and deployable as an API:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"predicted_category\": \"B\",\n",
    "  \"confidence\": 0.85,\n",
    "  \"reason\": \"Vitamin B12 is exclusively found in animal products, deficiency causes megaloblastic anemia...\"\n",
    "}\n",
    "```\n",
    "\n",
    "This is a **health industry agent** pattern: structured, auditable, and interoperable with downstream clinical systems."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "sample-question",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:16.898756Z",
     "iopub.status.busy": "2026-02-23T18:48:16.898496Z",
     "iopub.status.idle": "2026-02-23T18:48:16.913905Z",
     "shell.execute_reply": "2026-02-23T18:48:16.913454Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SYSTEM: You are a medical MCQ classifier. Given a medical multiple-choice question with  ...\n",
      "\n",
      "USER:\n",
      "Question: Which vitamin is exclusively found in animal products and its deficiency causes megaloblastic anemia?\n",
      "A) Vitamin B6\n",
      "B) Vitamin B12\n",
      "C) Folic acid\n",
      "D) Vitamin C\n",
      "\n",
      "ASSISTANT (target):\n",
      "{\n",
      "  \"predicted_category\": \"B\",\n",
      "  \"confidence\": 0.95,\n",
      "  \"reason\": \"Vitamin B12 (cobalamin) is exclusively found in animal products. Its deficiency causes megaloblastic anemia and subacute combined degeneration of the spinal cord.\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# Example MedMCQA question and the target JSON output\n",
    "example_question = {\n",
    "    \"question\": \"Which vitamin is exclusively found in animal products and its deficiency causes megaloblastic anemia?\",\n",
    "    \"opa\": \"Vitamin B6\",\n",
    "    \"opb\": \"Vitamin B12\",\n",
    "    \"opc\": \"Folic acid\",\n",
    "    \"opd\": \"Vitamin C\",\n",
    "    \"cop\": 1  # index 1 = option B\n",
    "}\n",
    "\n",
    "COP_MAP = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a medical MCQ classifier. \"\n",
    "    \"Given a medical multiple-choice question with 4 options (A, B, C, D), \"\n",
    "    \"analyze the question and respond with ONLY valid JSON in this exact format:\\n\"\n",
    "    '{\"predicted_category\": \"<A|B|C|D>\", \"confidence\": <float 0-1>, \"reason\": \"<concise explanation>\"}\\n'\n",
    "    \"Output nothing else ‚Äî no markdown, no extra text, just the JSON object.\"\n",
    ")\n",
    "\n",
    "user_msg = (\n",
    "    f\"Question: {example_question['question']}\\n\"\n",
    "    f\"A) {example_question['opa']}\\n\"\n",
    "    f\"B) {example_question['opb']}\\n\"\n",
    "    f\"C) {example_question['opc']}\\n\"\n",
    "    f\"D) {example_question['opd']}\"\n",
    ")\n",
    "\n",
    "# Ground-truth structured response\n",
    "target_response = json.dumps({\n",
    "    \"predicted_category\": COP_MAP[example_question[\"cop\"]],\n",
    "    \"confidence\": 0.95,\n",
    "    \"reason\": \"Vitamin B12 (cobalamin) is exclusively found in animal products. Its deficiency causes megaloblastic anemia and subacute combined degeneration of the spinal cord.\"\n",
    "}, indent=2)\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SYSTEM:\", SYSTEM_PROMPT[:80], \"...\")\n",
    "print()\n",
    "print(\"USER:\")\n",
    "print(user_msg)\n",
    "print()\n",
    "print(\"ASSISTANT (target):\")\n",
    "print(target_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pipeline-arch",
   "metadata": {},
   "source": [
    "## 2. Pipeline Architecture\n",
    "\n",
    "The full pipeline uses three **disjoint subsets** of MedMCQA (all sampled with `seed=42`):\n",
    "\n",
    "```\n",
    "openlifescienceai/medmcqa  (194k MCQs)\n",
    "            ‚îÇ\n",
    "    shuffle(seed=42) ‚Üí deterministic split\n",
    "            ‚îÇ\n",
    "   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îº‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "   ‚ñº        ‚ñº             ‚ñº\n",
    " idx 0-999  idx 1000-1499  idx 1500-1699\n",
    "  1,000      500            200\n",
    "  SFT set   GRPO set       EVAL set\n",
    "   ‚îÇ          ‚îÇ               ‚îÇ\n",
    "   ‚ñº          ‚ñº               ‚îÇ\n",
    "‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê   ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê           ‚îÇ\n",
    "‚îÇPhase1‚îÇ   ‚îÇPhase2‚îÇ           ‚îÇ\n",
    "‚îÇ  SFT ‚îÇ‚îÄ‚îÄ‚ñ∂‚îÇ GRPO ‚îÇ           ‚îÇ\n",
    "‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò   ‚îî‚îÄ‚îÄ‚î¨‚îÄ‚îÄ‚îÄ‚îò           ‚îÇ\n",
    "   ‚îÇ           ‚îÇ               ‚îÇ\n",
    "   ‚ñº           ‚ñº               ‚ñº\n",
    "wei25/      wei25/        ‚îå‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îê\n",
    "qwen3-0.6b  qwen3-0.6b   ‚îÇ Phase 3  ‚îÇ\n",
    "-medmcqa    -medmcqa-    ‚îÇ   EVAL   ‚îÇ\n",
    "-sft        grpo         ‚îÇ base/sft ‚îÇ\n",
    "                          ‚îÇ  /grpo   ‚îÇ\n",
    "                          ‚îî‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îÄ‚îò\n",
    "```\n",
    "\n",
    "No data leakage between phases ‚Äî each model is evaluated on samples it has **never seen during training**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "pipeline-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:16.916294Z",
     "iopub.status.busy": "2026-02-23T18:48:16.916107Z",
     "iopub.status.idle": "2026-02-23T18:48:17.370689Z",
     "shell.execute_reply": "2026-02-23T18:48:17.370439Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline diagram saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/8vnnfcld58sglqdf8sd9kt4c0000gn/T/ipykernel_10378/2143342576.py:68: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "from matplotlib.patches import FancyBboxPatch, FancyArrowPatch\n",
    "import numpy as np\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize=(14, 6))\n",
    "ax.set_xlim(0, 14)\n",
    "ax.set_ylim(0, 6)\n",
    "ax.axis('off')\n",
    "ax.set_facecolor('#f8f9fa')\n",
    "fig.patch.set_facecolor('#f8f9fa')\n",
    "\n",
    "def draw_box(ax, x, y, w, h, label, sublabel='', color='#4C72B0', text_color='white', fontsize=10):\n",
    "    box = FancyBboxPatch((x, y), w, h, boxstyle=\"round,pad=0.1\",\n",
    "                          facecolor=color, edgecolor='white', linewidth=1.5)\n",
    "    ax.add_patch(box)\n",
    "    ax.text(x + w/2, y + h/2 + (0.15 if sublabel else 0), label,\n",
    "            ha='center', va='center', fontsize=fontsize, fontweight='bold', color=text_color)\n",
    "    if sublabel:\n",
    "        ax.text(x + w/2, y + h/2 - 0.25, sublabel,\n",
    "                ha='center', va='center', fontsize=8, color=text_color, alpha=0.9)\n",
    "\n",
    "def draw_arrow(ax, x1, y1, x2, y2):\n",
    "    ax.annotate('', xy=(x2, y2), xytext=(x1, y1),\n",
    "                arrowprops=dict(arrowstyle='->', color='#555', lw=2))\n",
    "\n",
    "# Dataset\n",
    "draw_box(ax, 0.3, 2.2, 3.5, 1.6, 'MedMCQA', '194k Medical MCQs', color='#2d6a4f', fontsize=11)\n",
    "\n",
    "# Splits\n",
    "draw_box(ax, 1.0, 0.2, 1.0, 0.8, 'SFT Set', '1,000', color='#457b9d', fontsize=9)\n",
    "draw_box(ax, 2.2, 0.2, 1.0, 0.8, 'GRPO Set', '500', color='#e76f51', fontsize=9)\n",
    "draw_box(ax, 3.4, 0.2, 1.0, 0.8, 'Eval Set', '200', color='#6d6875', fontsize=9)\n",
    "\n",
    "# Arrows from dataset to splits\n",
    "draw_arrow(ax, 1.7, 2.2, 1.5, 1.0)\n",
    "draw_arrow(ax, 2.05, 2.2, 2.7, 1.0)\n",
    "draw_arrow(ax, 2.5, 2.2, 3.9, 1.0)\n",
    "\n",
    "# Phase boxes\n",
    "draw_box(ax, 5.0, 3.5, 2.5, 1.8, 'Phase 1: SFT', 'Cross-entropy loss\\nLoRA (r=16, Œ±=32)', color='#457b9d', fontsize=10)\n",
    "draw_box(ax, 8.0, 3.5, 2.5, 1.8, 'Phase 2: GRPO', 'Rule-based rewards\\nG=4 completions/prompt', color='#e76f51', fontsize=10)\n",
    "draw_box(ax, 5.5, 0.8, 4.0, 2.0, 'Phase 3: Eval', '200 held-out samples\\n3 models compared\\n4 metrics', color='#6d6875', fontsize=10)\n",
    "\n",
    "# Hub models\n",
    "draw_box(ax, 5.0, 5.6, 2.5, 0.7, 'HF Hub: -sft', '', color='#1d3557', fontsize=9)\n",
    "draw_box(ax, 8.0, 5.6, 2.5, 0.7, 'HF Hub: -grpo', '', color='#1d3557', fontsize=9)\n",
    "\n",
    "# Base model\n",
    "draw_box(ax, 5.0, 5.0, 2.5, 0.4, 'Qwen3-0.6B (base)', '', color='#333', fontsize=8)\n",
    "\n",
    "# Arrows\n",
    "draw_arrow(ax, 1.5, 1.0, 5.5, 3.5)    # SFT set ‚Üí Phase 1\n",
    "draw_arrow(ax, 2.7, 1.0, 8.5, 3.5)    # GRPO set ‚Üí Phase 2\n",
    "draw_arrow(ax, 3.9, 1.0, 7.5, 0.8)    # Eval set ‚Üí Phase 3\n",
    "draw_arrow(ax, 6.25, 5.3, 6.25, 5.3)  \n",
    "draw_arrow(ax, 6.25, 5.0, 6.25, 5.3)  # base ‚Üí SFT training\n",
    "draw_arrow(ax, 6.25, 3.5, 6.25, 3.5)  \n",
    "draw_arrow(ax, 7.5, 4.4, 8.0, 4.4)    # SFT ‚Üí GRPO\n",
    "draw_arrow(ax, 7.5, 5.95, 8.0, 5.95)  \n",
    "draw_arrow(ax, 6.25, 3.5, 6.25, 2.8)  # SFT ‚Üí eval\n",
    "draw_arrow(ax, 9.25, 3.5, 9.25, 2.8)  # GRPO ‚Üí eval\n",
    "draw_arrow(ax, 5.0, 5.2, 4.5, 2.8)    # base ‚Üí eval\n",
    "\n",
    "ax.set_title('MedMCQA Healthcare ML Pipeline', fontsize=14, fontweight='bold', pad=10)\n",
    "plt.tight_layout()\n",
    "plt.savefig('pipeline_diagram.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Pipeline diagram saved.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sft-theory",
   "metadata": {},
   "source": [
    "## 3. Phase 1: Supervised Fine-Tuning (SFT)\n",
    "\n",
    "### 3.1 What is SFT?\n",
    "\n",
    "SFT teaches the model **format + domain vocabulary** by showing it many (question ‚Üí target JSON) pairs. We minimize the **cross-entropy loss** over the assistant response tokens only:\n",
    "\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log \\pi_\\theta\\bigl(y_t \\mid y_{<t},\\, x\\bigr)$$\n",
    "\n",
    "Where:\n",
    "- $x$ = system prompt ‚äï user question (with options A‚ÄìD)\n",
    "- $y$ = target JSON string (ground truth)\n",
    "- $T$ = number of tokens in $y$\n",
    "- $\\pi_\\theta$ = model parameterised by $\\theta$ (base weights + LoRA adapters)\n",
    "\n",
    "The loss is computed **only on the assistant tokens** ‚Äî the model learns to predict the JSON answer, not to reproduce the question.\n",
    "\n",
    "---\n",
    "\n",
    "### 3.2 LoRA: Parameter-Efficient Fine-Tuning\n",
    "\n",
    "Training all 600M parameters is expensive. **LoRA** (Low-Rank Adaptation) decomposes the weight update as a low-rank product:\n",
    "\n",
    "$$W = W_0 + \\frac{\\alpha}{r} \\cdot B A \\qquad B \\in \\mathbb{R}^{d \\times r},\\; A \\in \\mathbb{R}^{r \\times k}$$\n",
    "\n",
    "- $W_0$ = frozen pre-trained weights\n",
    "- $B, A$ = small trainable matrices (initialised: $B=0$, $A \\sim \\mathcal{N}(0, \\sigma^2)$)\n",
    "- $r$ = rank (controls capacity, we use $r=16$)\n",
    "- $\\alpha$ = scaling factor (we use $\\alpha=32$)\n",
    "\n",
    "**Trainable parameters** $\\approx 2 \\cdot r \\cdot d \\cdot n_{\\text{layers}} \\ll$ total params\n",
    "\n",
    "| LoRA Config | Value |\n",
    "|---|---|\n",
    "| Rank `r` | 16 |\n",
    "| Alpha `Œ±` | 32 |\n",
    "| Dropout | 0.05 |\n",
    "| Target modules | `q_proj`, `k_proj`, `v_proj`, `o_proj` |\n",
    "| Trainable params | ~3.6M (0.6% of 600M) |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "lora-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.371790Z",
     "iopub.status.busy": "2026-02-23T18:48:17.371695Z",
     "iopub.status.idle": "2026-02-23T18:48:17.504164Z",
     "shell.execute_reply": "2026-02-23T18:48:17.503901Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/8vnnfcld58sglqdf8sd9kt4c0000gn/T/ipykernel_10378/2560336280.py:87: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as mpatches\n",
    "import numpy as np\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# === LEFT: LoRA Decomposition Diagram ===\n",
    "ax1.set_xlim(0, 10)\n",
    "ax1.set_ylim(0, 6)\n",
    "ax1.axis('off')\n",
    "ax1.set_facecolor('#f8f9fa')\n",
    "ax1.set_title('LoRA Weight Decomposition', fontsize=13, fontweight='bold')\n",
    "\n",
    "def draw_matrix(ax, x, y, w, h, label, sublabel='', color='#4C72B0', text_color='white'):\n",
    "    rect = mpatches.FancyBboxPatch((x, y), w, h,\n",
    "        boxstyle=\"round,pad=0.05\", facecolor=color, edgecolor='white', linewidth=2)\n",
    "    ax.add_patch(rect)\n",
    "    ax.text(x + w/2, y + h/2 + (0.2 if sublabel else 0),\n",
    "            label, ha='center', va='center', fontsize=11, fontweight='bold', color=text_color)\n",
    "    if sublabel:\n",
    "        ax.text(x + w/2, y + h/2 - 0.3,\n",
    "                sublabel, ha='center', va='center', fontsize=9, color=text_color, alpha=0.9)\n",
    "\n",
    "# W0 (large frozen matrix)\n",
    "draw_matrix(ax1, 0.3, 1.0, 2.5, 4.0, 'W‚ÇÄ', 'Frozen\\nd √ó k', color='#6c757d')\n",
    "\n",
    "# Plus sign\n",
    "ax1.text(3.1, 3.0, '+', ha='center', va='center', fontsize=24, fontweight='bold', color='#333')\n",
    "\n",
    "# B matrix (tall)\n",
    "draw_matrix(ax1, 3.6, 1.0, 1.0, 4.0, 'B', 'd √ó r', color='#e76f51')\n",
    "\n",
    "# @ sign\n",
    "ax1.text(5.0, 3.0, '@', ha='center', va='center', fontsize=20, fontweight='bold', color='#333')\n",
    "\n",
    "# A matrix (wide)\n",
    "draw_matrix(ax1, 5.3, 2.0, 3.5, 1.8, 'A', 'r √ó k', color='#457b9d')\n",
    "\n",
    "# Labels\n",
    "ax1.text(1.55, 0.6, 'Frozen', ha='center', fontsize=9, color='#6c757d')\n",
    "ax1.text(4.1, 0.6, 'Trainable', ha='center', fontsize=9, color='#e76f51')\n",
    "ax1.text(7.05, 1.8, 'Trainable', ha='center', fontsize=9, color='#457b9d')\n",
    "\n",
    "# Formula\n",
    "ax1.text(5.0, 0.2, 'W = W‚ÇÄ + (Œ±/r) ¬∑ B @ A     [r=16, Œ±=32]',\n",
    "         ha='center', fontsize=11, color='#333',\n",
    "         bbox=dict(boxstyle='round', facecolor='#e8f4f8', edgecolor='#457b9d', linewidth=1.5))\n",
    "\n",
    "# Param count comparison\n",
    "ax1.text(5.0, 5.5, '~3.6M trainable  vs  600M total  (0.6%)',\n",
    "         ha='center', fontsize=10, color='#2d6a4f', fontweight='bold')\n",
    "\n",
    "# === RIGHT: SFT Hyperparameters ===\n",
    "ax2.axis('off')\n",
    "ax2.set_facecolor('#f8f9fa')\n",
    "ax2.set_title('SFT Training Configuration', fontsize=13, fontweight='bold')\n",
    "\n",
    "params = [\n",
    "    ('max_steps', '800', '‚âà3.2 epochs over 1,000 samples'),\n",
    "    ('learning_rate', '2e-4', 'cosine schedule'),\n",
    "    ('warmup_ratio', '0.03', '~24 warmup steps'),\n",
    "    ('batch_size (device)', '4', ''),\n",
    "    ('grad_accumulation', '4', 'effective batch = 16'),\n",
    "    ('max_length', '1024 tokens', ''),\n",
    "    ('precision', 'bf16', 'bfloat16'),\n",
    "    ('grad_checkpointing', 'True', 'saves ~40% VRAM'),\n",
    "    ('LoRA rank / alpha', '16 / 32', 'dropout=0.05'),\n",
    "    ('hardware', 't4-small', 'NVIDIA T4 16GB'),\n",
    "    ('estimated cost', '~$0.25', '~20 min runtime'),\n",
    "]\n",
    "\n",
    "colors = ['#e8f4f8', '#f0f4e8'] * 10\n",
    "for i, (param, value, note) in enumerate(params):\n",
    "    y = 5.2 - i * 0.47\n",
    "    ax2.add_patch(mpatches.FancyBboxPatch((0.1, y - 0.18), 9.8, 0.38,\n",
    "                  boxstyle='round,pad=0.05', facecolor=colors[i], edgecolor='none'))\n",
    "    ax2.text(0.4, y + 0.02, param, fontsize=10, fontweight='bold', color='#333', va='center')\n",
    "    ax2.text(4.5, y + 0.02, value, fontsize=10, color='#1d3557', va='center', fontweight='bold')\n",
    "    if note:\n",
    "        ax2.text(7.0, y + 0.02, f'({note})', fontsize=8, color='#666', va='center', style='italic')\n",
    "\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 5.8)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('sft_config.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sft-code-section",
   "metadata": {},
   "source": [
    "### 3.3 SFT Training Script (Key Parts)\n",
    "\n",
    "The full script is `Fine-Tune/train_medmcqa_sft.py` ‚Äî a PEP 723 UV script that runs on HF Jobs.\n",
    "\n",
    "**Data format** ‚Äî each example becomes a chat-messages list:\n",
    "\n",
    "```python\n",
    "[\n",
    "  {\"role\": \"system\",    \"content\": \"<classifier instructions>\"},\n",
    "  {\"role\": \"user\",      \"content\": \"Question: ...\\nA) ...\\nB) ...\"},\n",
    "  {\"role\": \"assistant\", \"content\": '{\"predicted_category\": \"C\", \"confidence\": 0.95, ...}'}\n",
    "]\n",
    "```\n",
    "\n",
    "**Launching on HF Jobs:**\n",
    "\n",
    "```bash\n",
    "# NOTE: --flavor and --timeout must come BEFORE the script filename\n",
    "hf jobs uv run --flavor t4-small --timeout 2h Fine-Tune/train_medmcqa_sft.py\n",
    "```\n",
    "\n",
    "**SFT Result** ‚Äî Job `699b9aa152d1c53b7df7d387` completed ~512/800 steps (hit 1h timeout). Despite the early stop, the model showed meaningful accuracy improvement."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "sft-code-demo",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.505439Z",
     "iopub.status.busy": "2026-02-23T18:48:17.505360Z",
     "iopub.status.idle": "2026-02-23T18:48:17.508077Z",
     "shell.execute_reply": "2026-02-23T18:48:17.507792Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Formatted training example:\n",
      "[\n",
      "  {\n",
      "    \"role\": \"system\",\n",
      "    \"content\": \"You are a medical MCQ classifier. Given a medical multiple-choice question with 4 options (A, B, C, D), analyze the question and respond with ONLY valid JSON in this exact format:\\n{\\\"predicted_category\\\": \\\"<A|B|C|D>\\\", \\\"confidence\\\": <float 0-1>, \\\"reason\\\": \\\"<concise explanation>\\\"}\\nOutput nothing else \\u2014 no markdown, no extra text, just the JSON object.\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"user\",\n",
      "    \"content\": \"Question: Which of the following is the most common cause of pneumonia in adults?\\nA) Haemophilus influenzae\\nB) Streptococcus pneumoniae\\nC) Klebsiella pneumoniae\\nD) Staphylococcus aureus\"\n",
      "  },\n",
      "  {\n",
      "    \"role\": \"assistant\",\n",
      "    \"content\": \"{\\\"predicted_category\\\": \\\"B\\\", \\\"confidence\\\": 0.95, \\\"reason\\\": \\\"Streptococcus pneumoniae (pneumococcus) is the most common cause of community-acquired pneumonia in adults worldwide.\\\"}\"\n",
      "  }\n",
      "]\n"
     ]
    }
   ],
   "source": [
    "# Key data formatting and training setup (illustrative ‚Äî not for execution here)\n",
    "import json\n",
    "\n",
    "SYSTEM_PROMPT = (\n",
    "    \"You are a medical MCQ classifier. \"\n",
    "    \"Given a medical multiple-choice question with 4 options (A, B, C, D), \"\n",
    "    \"analyze the question and respond with ONLY valid JSON in this exact format:\\n\"\n",
    "    '{\"predicted_category\": \"<A|B|C|D>\", \"confidence\": <float 0-1>, \"reason\": \"<concise explanation>\"}\\n'\n",
    "    \"Output nothing else ‚Äî no markdown, no extra text, just the JSON object.\"\n",
    ")\n",
    "\n",
    "COP_MAP = {0: \"A\", 1: \"B\", 2: \"C\", 3: \"D\"}\n",
    "\n",
    "def format_to_chat(example):\n",
    "    \"\"\"Convert a MedMCQA row into a chat-messages list for SFT.\"\"\"\n",
    "    user_msg = (\n",
    "        f\"Question: {example['question']}\\n\"\n",
    "        f\"A) {example['opa']}\\n\"\n",
    "        f\"B) {example['opb']}\\n\"\n",
    "        f\"C) {example['opc']}\\n\"\n",
    "        f\"D) {example['opd']}\"\n",
    "    )\n",
    "    answer_letter = COP_MAP[example[\"cop\"]]\n",
    "    reason = (example.get(\"exp\") or \"Based on medical knowledge and clinical reasoning.\")\n",
    "    if len(reason) > 250:\n",
    "        reason = reason[:247] + \"...\"\n",
    "\n",
    "    assistant_msg = json.dumps({\n",
    "        \"predicted_category\": answer_letter,\n",
    "        \"confidence\": 0.95,\n",
    "        \"reason\": reason,\n",
    "    }, ensure_ascii=False)\n",
    "\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\",    \"content\": SYSTEM_PROMPT},\n",
    "            {\"role\": \"user\",      \"content\": user_msg},\n",
    "            {\"role\": \"assistant\", \"content\": assistant_msg},\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Demo: format a sample\n",
    "sample = {\n",
    "    \"question\": \"Which of the following is the most common cause of pneumonia in adults?\",\n",
    "    \"opa\": \"Haemophilus influenzae\",\n",
    "    \"opb\": \"Streptococcus pneumoniae\",\n",
    "    \"opc\": \"Klebsiella pneumoniae\",\n",
    "    \"opd\": \"Staphylococcus aureus\",\n",
    "    \"cop\": 1,\n",
    "    \"exp\": \"Streptococcus pneumoniae (pneumococcus) is the most common cause of community-acquired pneumonia in adults worldwide.\"\n",
    "}\n",
    "\n",
    "formatted = format_to_chat(sample)\n",
    "print(\"Formatted training example:\")\n",
    "print(json.dumps(formatted[\"messages\"], indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grpo-theory",
   "metadata": {},
   "source": [
    "## 4. Phase 2: Group Relative Policy Optimization (GRPO)\n",
    "\n",
    "### 4.1 Why GRPO After SFT?\n",
    "\n",
    "SFT teaches the model to **imitate** fixed ground-truth answers ‚Äî it's great for format and vocabulary, but limited to the training examples.\n",
    "\n",
    "**GRPO** (inspired by [DeepSeek-R1](https://arxiv.org/abs/2501.12948)) lets the model:\n",
    "1. **Explore** ‚Äî generate multiple candidate answers per question\n",
    "2. **Self-improve** ‚Äî learn from its own correct answers (no reward model needed!)\n",
    "3. **Sharpen** ‚Äî directly optimise for accuracy + JSON compliance via rule-based rewards\n",
    "\n",
    "---\n",
    "\n",
    "### 4.2 The GRPO Algorithm\n",
    "\n",
    "**Step 1: Generate $G$ completions per prompt**\n",
    "\n",
    "For a prompt $q$, sample $G=4$ completions from the current policy:\n",
    "$$\\{o_1, o_2, o_3, o_4\\} \\sim \\pi_\\theta(\\cdot \\mid q)$$\n",
    "\n",
    "**Step 2: Score with rule-based rewards**\n",
    "\n",
    "Each completion gets a reward $r_i \\in [0, 3]$:\n",
    "$$R = R_{\\text{json}} + R_{\\text{schema}} + R_{\\text{accuracy}} \\in \\{0, 1, 2, 3\\}$$\n",
    "\n",
    "**Step 3: Compute group-relative advantage**\n",
    "\n",
    "Normalise within the group ‚Äî no baseline critic needed:\n",
    "$$\\hat{A}_i = \\frac{r_i - \\text{mean}(r_1, \\ldots, r_G)}{\\text{std}(r_1, \\ldots, r_G)}$$\n",
    "\n",
    "**Step 4: Optimise the clipped surrogate objective**\n",
    "\n",
    "$$\\mathcal{L}_{\\text{GRPO}}(\\theta) = -\\frac{1}{\\sum_i |o_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|o_i|} \\min\\!\\left( \\rho_{i,t} \\hat{A}_i,\\; \\text{clip}(\\rho_{i,t}, 1{-}\\varepsilon, 1{+}\\varepsilon) \\hat{A}_i \\right)$$\n",
    "\n",
    "Where $\\rho_{i,t} = \\dfrac{\\pi_\\theta(o_{i,t} \\mid q, o_{i,<t})}{\\pi_{\\text{ref}}(o_{i,t} \\mid q, o_{i,<t})}$ is the importance sampling ratio.\n",
    "\n",
    "**Key insight**: Completions that score _above_ the group average ($\\hat{A}_i > 0$) are reinforced. Completions that score _below_ average ($\\hat{A}_i < 0$) are suppressed. **No learned reward model needed.**\n",
    "\n",
    "---\n",
    "\n",
    "### 4.3 Rule-Based Reward Functions\n",
    "\n",
    "Three binary rewards, summed:\n",
    "\n",
    "| Reward | Criteria | Value |\n",
    "|---|---|---|\n",
    "| $R_{\\text{json}}$ | `json.loads(output)` succeeds | 0 or 1 |\n",
    "| $R_{\\text{schema}}$ | Has `predicted_category` ‚àà {A,B,C,D}, `confidence` ‚àà [0,1], non-empty `reason` | 0 or 1 |\n",
    "| $R_{\\text{accuracy}}$ | `predicted_category` matches ground truth | 0 or 1 |\n",
    "\n",
    "Perfect answer = `3.0` ¬∑ Valid format, wrong answer = `2.0` ¬∑ Broken JSON = `0.0`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "grpo-rewards",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.509324Z",
     "iopub.status.busy": "2026-02-23T18:48:17.509260Z",
     "iopub.status.idle": "2026-02-23T18:48:17.512780Z",
     "shell.execute_reply": "2026-02-23T18:48:17.512585Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Completion                                         R_json R_schema  R_acc R_total\n",
      "--------------------------------------------------------------------------------\n",
      "Perfect JSON, correct                                 1.0      1.0    1.0     3.0\n",
      "Valid JSON, wrong                                     1.0      1.0    0.0     2.0\n",
      "Valid JSON, missing schema                            1.0      0.0    1.0     2.0\n",
      "Invalid JSON                                          0.0      0.0    0.0     0.0\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ‚îÄ‚îÄ The three rule-based reward functions used in GRPO training ‚îÄ‚îÄ\n",
    "\n",
    "def _extract_text(completion):\n",
    "    \"\"\"Handle both string and chat-format completions.\"\"\"\n",
    "    if isinstance(completion, list):\n",
    "        return completion[-1][\"content\"] if completion else \"\"\n",
    "    return str(completion)\n",
    "\n",
    "\n",
    "def json_validity_reward(completions, **kwargs):\n",
    "    \"\"\"R1: Can the output be parsed as valid JSON?  ‚Üí 0 or 1\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = _extract_text(completion)\n",
    "        try:\n",
    "            json.loads(text)\n",
    "            rewards.append(1.0)\n",
    "        except (json.JSONDecodeError, TypeError):\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def schema_compliance_reward(completions, **kwargs):\n",
    "    \"\"\"R2: Does the JSON have the 3 required keys with correct types?  ‚Üí 0 or 1\"\"\"\n",
    "    rewards = []\n",
    "    for completion in completions:\n",
    "        text = _extract_text(completion)\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "            has_cat   = obj.get(\"predicted_category\") in (\"A\", \"B\", \"C\", \"D\")\n",
    "            conf      = obj.get(\"confidence\")\n",
    "            has_conf  = isinstance(conf, (int, float)) and 0 <= conf <= 1\n",
    "            reason    = obj.get(\"reason\")\n",
    "            has_reason = isinstance(reason, str) and len(reason.strip()) > 0\n",
    "            rewards.append(1.0 if (has_cat and has_conf and has_reason) else 0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "def accuracy_reward(completions, ground_truth, **kwargs):\n",
    "    \"\"\"R3: Does predicted_category match the gold label?  ‚Üí 0 or 1\"\"\"\n",
    "    rewards = []\n",
    "    for completion, gt in zip(completions, ground_truth):\n",
    "        text = _extract_text(completion)\n",
    "        try:\n",
    "            obj = json.loads(text)\n",
    "            rewards.append(1.0 if obj.get(\"predicted_category\") == gt else 0.0)\n",
    "        except Exception:\n",
    "            rewards.append(0.0)\n",
    "    return rewards\n",
    "\n",
    "\n",
    "# ‚îÄ‚îÄ Test the reward functions with example completions ‚îÄ‚îÄ\n",
    "completions = [\n",
    "    '{\"predicted_category\": \"B\", \"confidence\": 0.9, \"reason\": \"Vitamin B12 is in animals\"}',  # perfect\n",
    "    '{\"predicted_category\": \"A\", \"confidence\": 0.6, \"reason\": \"wrong answer\"}',               # valid JSON, wrong answer\n",
    "    '{\"predicted_category\": \"B\"}',                                                             # valid JSON, missing schema\n",
    "    'This is the answer: B',                                                                   # invalid JSON\n",
    "]\n",
    "ground_truth = [\"B\", \"B\", \"B\", \"B\"]\n",
    "\n",
    "r_json = json_validity_reward(completions)\n",
    "r_schema = schema_compliance_reward(completions)\n",
    "r_acc = accuracy_reward(completions, ground_truth)\n",
    "r_total = [r_json[i] + r_schema[i] + r_acc[i] for i in range(len(completions))]\n",
    "\n",
    "print(f\"{'Completion':<50} {'R_json':>6} {'R_schema':>8} {'R_acc':>6} {'R_total':>7}\")\n",
    "print(\"-\" * 80)\n",
    "labels = [\"Perfect JSON, correct\", \"Valid JSON, wrong\", \"Valid JSON, missing schema\", \"Invalid JSON\"]\n",
    "for label, rj, rs, ra, rt in zip(labels, r_json, r_schema, r_acc, r_total):\n",
    "    print(f\"{label:<50} {rj:>6.1f} {rs:>8.1f} {ra:>6.1f} {rt:>7.1f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "grpo-advantage-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.513754Z",
     "iopub.status.busy": "2026-02-23T18:48:17.513685Z",
     "iopub.status.idle": "2026-02-23T18:48:17.664483Z",
     "shell.execute_reply": "2026-02-23T18:48:17.664238Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/8vnnfcld58sglqdf8sd9kt4c0000gn/T/ipykernel_10378/3826109807.py:76: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(13, 5))\n",
    "fig.suptitle('GRPO: Group Relative Policy Optimization', fontsize=14, fontweight='bold')\n",
    "\n",
    "# === LEFT: Reward and Advantage for one step ===\n",
    "ax1.set_title('Group-Relative Advantage Calculation\\n(G=4 completions per prompt)', fontsize=11)\n",
    "\n",
    "rewards = np.array([3.0, 2.0, 0.0, 1.0])  # example group rewards\n",
    "labels_comp = ['o‚ÇÅ: Perfect\\n(JSON‚úì Schema‚úì Correct‚úì)',\n",
    "               'o‚ÇÇ: Valid, Wrong\\n(JSON‚úì Schema‚úì Correct‚úó)',\n",
    "               'o‚ÇÉ: Broken JSON\\n(JSON‚úó Schema‚úó Correct‚úó)',\n",
    "               'o‚ÇÑ: Partial\\n(JSON‚úì Schema‚úó Correct‚úó)']\n",
    "\n",
    "r_mean = rewards.mean()\n",
    "r_std = rewards.std()\n",
    "advantages = (rewards - r_mean) / r_std\n",
    "\n",
    "colors_comp = ['#2d6a4f' if a > 0 else '#c1121f' for a in advantages]\n",
    "\n",
    "bars = ax1.barh(range(4), rewards, color=['#457b9d']*4, alpha=0.7, label='Reward r_i', height=0.4)\n",
    "\n",
    "# Annotate\n",
    "for i, (r, a) in enumerate(zip(rewards, advantages)):\n",
    "    ax1.text(r + 0.05, i, f'  r={r:.0f}  ‚Üí  √Ç={a:+.2f}',\n",
    "             va='center', fontsize=10,\n",
    "             color='#2d6a4f' if a > 0 else '#c1121f',\n",
    "             fontweight='bold')\n",
    "\n",
    "ax1.axvline(r_mean, color='orange', linewidth=2, linestyle='--', label=f'Group mean = {r_mean:.1f}')\n",
    "ax1.set_yticks(range(4))\n",
    "ax1.set_yticklabels(labels_comp, fontsize=9)\n",
    "ax1.set_xlabel('Reward R = R_json + R_schema + R_accuracy', fontsize=10)\n",
    "ax1.set_xlim(0, 5.5)\n",
    "ax1.legend(fontsize=9)\n",
    "ax1.text(0.02, 0.02,\n",
    "         f'mean={r_mean:.2f}, std={r_std:.2f}\\n√Ç_i = (r_i - mean) / std',\n",
    "         transform=ax1.transAxes, fontsize=9,\n",
    "         bbox=dict(boxstyle='round', facecolor='#e8f4f8', alpha=0.8))\n",
    "\n",
    "# === RIGHT: GRPO hyperparameters ===\n",
    "ax2.axis('off')\n",
    "ax2.set_title('GRPO Training Configuration', fontsize=11)\n",
    "\n",
    "grpo_params = [\n",
    "    ('Starting model', 'wei25/qwen3-0.6b-medmcqa-sft', ''),\n",
    "    ('max_steps', '400', 'RL is more sample-efficient'),\n",
    "    ('learning_rate', '5e-6', '40√ó lower than SFT'),\n",
    "    ('batch_size (device)', '2', 'G√ó2 completions'),\n",
    "    ('grad_accumulation', '4', 'effective = 8 prompts'),\n",
    "    ('num_generations (G)', '4', 'completions per prompt'),\n",
    "    ('max_completion_length', '256 tokens', ''),\n",
    "    ('clip epsilon (Œµ)', '0.2', 'PPO-style clipping'),\n",
    "    ('hardware', 't4-small', 'NVIDIA T4 16GB'),\n",
    "    ('estimated cost', '~$0.50', '~40 min runtime'),\n",
    "]\n",
    "\n",
    "bg_colors = ['#e8f4f8', '#f4e8f4'] * 10\n",
    "for i, (param, value, note) in enumerate(grpo_params):\n",
    "    y = 5.5 - i * 0.53\n",
    "    ax2.add_patch(plt.matplotlib.patches.FancyBboxPatch(\n",
    "        (0.0, y - 0.22), 10.0, 0.44,\n",
    "        boxstyle='round,pad=0.04', facecolor=bg_colors[i], edgecolor='none'\n",
    "    ))\n",
    "    ax2.text(0.2, y + 0.01, param, fontsize=9.5, fontweight='bold', color='#333', va='center')\n",
    "    ax2.text(4.5, y + 0.01, value, fontsize=9.5, color='#1d3557', va='center', fontweight='bold')\n",
    "    if note:\n",
    "        ax2.text(7.8, y + 0.01, f'({note})', fontsize=8, color='#666', va='center', style='italic')\n",
    "\n",
    "ax2.set_xlim(0, 10)\n",
    "ax2.set_ylim(0, 6)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('grpo_config.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eval-theory",
   "metadata": {},
   "source": [
    "## 5. Phase 3: Evaluation\n",
    "\n",
    "### 5.1 Held-Out Test Set\n",
    "\n",
    "- **200 samples** from MedMCQA (indices 1500‚Äì1699 after `shuffle(seed=42)`)\n",
    "- Completely **disjoint** from SFT (0‚Äì999) and GRPO (1000‚Äì1499) training data\n",
    "- Inference: **greedy decoding** (`do_sample=False`), `max_new_tokens=128`\n",
    "\n",
    "### 5.2 Models Evaluated\n",
    "\n",
    "| Label | Model | Description |\n",
    "|---|---|---|\n",
    "| `base` | `Qwen/Qwen3-0.6B` | Zero-shot baseline |\n",
    "| `sft` | `wei25/qwen3-0.6b-medmcqa-sft` | After Phase 1 |\n",
    "| `grpo` | `wei25/qwen3-0.6b-medmcqa-grpo` | After Phase 2 |\n",
    "\n",
    "### 5.3 Four Evaluation Metrics\n",
    "\n",
    "$$\\text{accuracy} = \\frac{\\text{correct}}{\\text{total}} \\qquad \\text{valid\\_json\\_rate} = \\frac{\\text{json\\_parseable}}{\\text{total}}$$\n",
    "\n",
    "$$\\text{schema\\_pass\\_rate} = \\frac{\\text{schema\\_valid}}{\\text{total}} \\qquad \\text{accuracy\\_on\\_schema} = \\frac{\\text{correct} \\cap \\text{schema\\_valid}}{\\text{schema\\_valid}}$$\n",
    "\n",
    "Where:\n",
    "- `correct` ‚Äî `predicted_category` matches ground truth\n",
    "- `json_parseable` ‚Äî `json.loads(output)` succeeds\n",
    "- `schema_valid` ‚Äî JSON parseable **AND** `predicted_category` ‚àà {A,B,C,D} **AND** `confidence` ‚àà [0,1] **AND** `reason` is non-empty\n",
    "\n",
    "> **`accuracy_on_schema`** isolates _\"does the model know medicine?\"_ from _\"can it follow the format?\"_\n",
    "\n",
    "### 5.4 Key Inference Fixes for Qwen3\n",
    "\n",
    "```python\n",
    "# Disable Qwen3's <think> chain-of-thought blocks (3-5x slower)\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages, tokenize=False, add_generation_prompt=True,\n",
    "    enable_thinking=False,  # ‚Üê critical for Qwen3\n",
    ")\n",
    "\n",
    "# Detect LoRA adapter repos (vs full model repos)\n",
    "def _is_peft_adapter(model_id):\n",
    "    return \"adapter_config.json\" in list_repo_files(model_id)\n",
    "\n",
    "# Load adapter + merge weights before inference\n",
    "if _is_peft_adapter(model_id):\n",
    "    model = AutoPeftModelForCausalLM.from_pretrained(model_id, ...).merge_and_unload()\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "results-section",
   "metadata": {},
   "source": [
    "## 6. Results\n",
    "\n",
    "Evaluation job `699be2881aad19adb8aad4d9` ‚Äî completed on `t4-small` in ~10 min.\n",
    "\n",
    "| Metric | base | sft | grpo | Delta (SFT vs base) |\n",
    "|---|---|---|---|---|\n",
    "| **accuracy** | 27.5% | **34.0%** | 27.5% | **+6.5 pp** ‚úÖ |\n",
    "| valid_json_rate | 98.5% | 98.5% | 98.5% | 0 pp |\n",
    "| schema_pass_rate | 95.5% | **98.5%** | 95.5% | **+3.0 pp** ‚úÖ |\n",
    "| accuracy_on_schema | 28.8% | **34.5%** | 28.8% | **+5.7 pp** ‚úÖ |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "results-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.665818Z",
     "iopub.status.busy": "2026-02-23T18:48:17.665737Z",
     "iopub.status.idle": "2026-02-23T18:48:17.872948Z",
     "shell.execute_reply": "2026-02-23T18:48:17.872718Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results chart saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/8vnnfcld58sglqdf8sd9kt4c0000gn/T/ipykernel_10378/154281136.py:59: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# ‚îÄ‚îÄ Actual evaluation results ‚îÄ‚îÄ\n",
    "results = {\n",
    "    \"base\":  {\"accuracy\": 0.275, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.955, \"accuracy_on_schema\": 0.288},\n",
    "    \"sft\":   {\"accuracy\": 0.340, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.985, \"accuracy_on_schema\": 0.345},\n",
    "    \"grpo\":  {\"accuracy\": 0.275, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.955, \"accuracy_on_schema\": 0.288},\n",
    "}\n",
    "\n",
    "metrics = [\"accuracy\", \"valid_json_rate\", \"schema_pass_rate\", \"accuracy_on_schema\"]\n",
    "metric_labels = [\"Accuracy\\n(overall)\", \"Valid JSON Rate\", \"Schema Pass Rate\", \"Accuracy\\non Schema\"]\n",
    "model_labels = [\"base\", \"sft\", \"grpo\"]\n",
    "colors = {'base': '#6c757d', 'sft': '#457b9d', 'grpo': '#e76f51'}\n",
    "\n",
    "fig, axes = plt.subplots(1, 4, figsize=(16, 5.5))\n",
    "fig.suptitle('MedMCQA Evaluation Results ‚Äî 200 Held-Out Samples (Greedy Decoding)',\n",
    "             fontsize=13, fontweight='bold', y=1.02)\n",
    "\n",
    "x = np.arange(len(model_labels))\n",
    "\n",
    "for ax, metric, mlabel in zip(axes, metrics, metric_labels):\n",
    "    values = [results[m][metric] for m in model_labels]\n",
    "    bar_colors = [colors[m] for m in model_labels]\n",
    "    bars = ax.bar(x, [v * 100 for v in values], color=bar_colors, alpha=0.85,\n",
    "                  edgecolor='white', linewidth=1.5, width=0.55)\n",
    "\n",
    "    # Value labels\n",
    "    for bar, val in zip(bars, values):\n",
    "        ax.text(bar.get_x() + bar.get_width()/2, bar.get_height() + 0.4,\n",
    "                f'{val*100:.1f}%', ha='center', va='bottom', fontsize=10, fontweight='bold')\n",
    "\n",
    "    # Highlight best bar\n",
    "    best_idx = int(np.argmax(values))\n",
    "    bars[best_idx].set_edgecolor('#f4a261')\n",
    "    bars[best_idx].set_linewidth(3)\n",
    "\n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels(model_labels, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylabel('Score (%)', fontsize=10)\n",
    "    ax.set_title(mlabel, fontsize=11, fontweight='bold')\n",
    "    ax.set_ylim(0, 115)\n",
    "    ax.yaxis.grid(True, alpha=0.3)\n",
    "    ax.set_axisbelow(True)\n",
    "\n",
    "    # Chance level for accuracy plots\n",
    "    if 'accuracy' in metric:\n",
    "        ax.axhline(25, color='red', linewidth=1.2, linestyle='--', alpha=0.5, label='Random (25%)')\n",
    "        ax.legend(fontsize=8, loc='lower right')\n",
    "\n",
    "# Custom legend\n",
    "legend_patches = [plt.Rectangle((0,0),1,1, color=c, alpha=0.85) for c in colors.values()]\n",
    "fig.legend(legend_patches, ['Base (Qwen3-0.6B)', 'SFT (Phase 1)', 'GRPO (Phase 2)'],\n",
    "           loc='lower center', ncol=3, fontsize=11, bbox_to_anchor=(0.5, -0.08),\n",
    "           framealpha=0.9)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('eval_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()\n",
    "print(\"Results chart saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "delta-viz",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:17.873973Z",
     "iopub.status.busy": "2026-02-23T18:48:17.873895Z",
     "iopub.status.idle": "2026-02-23T18:48:18.011856Z",
     "shell.execute_reply": "2026-02-23T18:48:18.011584Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/l4/8vnnfcld58sglqdf8sd9kt4c0000gn/T/ipykernel_10378/1554061669.py:51: UserWarning: FigureCanvasAgg is non-interactive, and thus cannot be shown\n",
      "  plt.show()\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "# Delta plot: improvement vs base\n",
    "results = {\n",
    "    \"base\":  {\"accuracy\": 0.275, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.955, \"accuracy_on_schema\": 0.288},\n",
    "    \"sft\":   {\"accuracy\": 0.340, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.985, \"accuracy_on_schema\": 0.345},\n",
    "    \"grpo\":  {\"accuracy\": 0.275, \"valid_json_rate\": 0.985, \"schema_pass_rate\": 0.955, \"accuracy_on_schema\": 0.288},\n",
    "}\n",
    "\n",
    "metrics = [\"accuracy\", \"valid_json_rate\", \"schema_pass_rate\", \"accuracy_on_schema\"]\n",
    "metric_labels = [\"Accuracy\", \"Valid JSON Rate\", \"Schema Pass Rate\", \"Accuracy on Schema\"]\n",
    "\n",
    "sft_delta = [(results[\"sft\"][m] - results[\"base\"][m]) * 100 for m in metrics]\n",
    "grpo_delta = [(results[\"grpo\"][m] - results[\"base\"][m]) * 100 for m in metrics]\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(10, 4.5))\n",
    "\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.35\n",
    "\n",
    "bars1 = ax.bar(x - width/2, sft_delta, width, label='SFT vs Base',\n",
    "               color=['#2d6a4f' if v >= 0 else '#c1121f' for v in sft_delta], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "bars2 = ax.bar(x + width/2, grpo_delta, width, label='GRPO vs Base',\n",
    "               color=['#e9c46a' if v >= 0 else '#c1121f' for v in grpo_delta], alpha=0.85, edgecolor='white', linewidth=1.5)\n",
    "\n",
    "# Value labels\n",
    "for bar in bars1:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + 0.05 if h >= 0 else h - 0.4,\n",
    "            f'{h:+.1f}pp', ha='center', va='bottom' if h >= 0 else 'top',\n",
    "            fontsize=9.5, fontweight='bold')\n",
    "for bar in bars2:\n",
    "    h = bar.get_height()\n",
    "    ax.text(bar.get_x() + bar.get_width()/2, h + 0.05 if h >= 0 else h - 0.4,\n",
    "            f'{h:+.1f}pp', ha='center', va='bottom' if h >= 0 else 'top',\n",
    "            fontsize=9.5, fontweight='bold')\n",
    "\n",
    "ax.axhline(0, color='black', linewidth=1.2)\n",
    "ax.set_xticks(x)\n",
    "ax.set_xticklabels(metric_labels, fontsize=11)\n",
    "ax.set_ylabel('Œî vs Base (percentage points)', fontsize=11)\n",
    "ax.set_title('Model Improvements over Base ‚Äî Phase 1 (SFT) and Phase 2 (GRPO)', fontsize=12, fontweight='bold')\n",
    "ax.legend(fontsize=11)\n",
    "ax.yaxis.grid(True, alpha=0.3)\n",
    "ax.set_axisbelow(True)\n",
    "ax.set_ylim(-2, 10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig('delta_results.png', dpi=150, bbox_inches='tight')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "findings",
   "metadata": {},
   "source": [
    "## 7. Key Findings & Analysis\n",
    "\n",
    "### ‚úÖ Finding 1: SFT Meaningfully Improved Accuracy (+6.5 pp)\n",
    "\n",
    "Training on just **1,000 examples** with the JSON format ‚Äî even with an early stop at ~512/800 steps ‚Äî boosted accuracy from 27.5% ‚Üí 34.0%. The model learned to align its medical knowledge with the target output schema.\n",
    "\n",
    "Random chance baseline = **25%** (4 options). Both base and SFT beat chance significantly; SFT by a wider margin.\n",
    "\n",
    "---\n",
    "\n",
    "### ‚úÖ Finding 2: Qwen3-0.6B is Already an Excellent JSON Outputter\n",
    "\n",
    "The **base model** (zero-shot, no fine-tuning) achieved:\n",
    "- `valid_json_rate = 98.5%` ‚Äî nearly perfect JSON compliance out of the box\n",
    "- `schema_pass_rate = 95.5%` ‚Äî already follows the 3-key schema without training\n",
    "\n",
    "This is a property of **Qwen3's pre-training** on structured data. SFT pushed schema pass rate to 98.5% (eliminating most of the remaining failures).\n",
    "\n",
    "---\n",
    "\n",
    "### ‚ö†Ô∏è Finding 3: GRPO Reward Collapse\n",
    "\n",
    "GRPO training produced **all rewards = 0.0** throughout (`frac_reward_zero_std = 1.0`). The root cause:\n",
    "\n",
    "```\n",
    "GRPO rollout phase:\n",
    "  The GRPO trainer generates completions from the SFT model at inference time.\n",
    "  If the SFT model (partially trained, ~512/800 steps) fails to reliably\n",
    "  produce valid JSON during GRPO rollouts, ALL 4 completions score R=0.\n",
    "  With std(r) = 0, the group-relative advantage is undefined ‚Üí no learning signal.\n",
    "```\n",
    "\n",
    "**Why GRPO eval == base performance:**\n",
    "\n",
    "```\n",
    "GRPO saved a LoRA adapter.\n",
    "adapter_config.json records:\n",
    "  base_model_name_or_path = \"Qwen/Qwen3-0.6B\"   ‚Üê not the SFT merged model!\n",
    "\n",
    "At eval time:\n",
    "  AutoPeftModelForCausalLM.from_pretrained(\"wei25/qwen3-0.6b-medmcqa-grpo\")\n",
    "  ‚Üí loads Qwen3-0.6B base\n",
    "  ‚Üí applies near-zero GRPO LoRA weights (all rewards were 0, no learning)\n",
    "  ‚Üí effectively evaluates the base model\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "### üîß Lesson Learned: Fix for Next Iteration\n",
    "\n",
    "| Issue | Fix |\n",
    "|---|---|\n",
    "| SFT cut short at 1h | Use `--timeout 2h` to complete all 800 steps |\n",
    "| GRPO rewards all zero | Full SFT model ‚Üí reliable JSON ‚Üí non-zero rewards |\n",
    "| GRPO adapter chain broken | Push merged model at end of GRPO (not LoRA adapter) |\n",
    "\n",
    "**The correct pipeline for chaining:**\n",
    "```python\n",
    "# At end of GRPO training script:\n",
    "merged_model = trainer.model.merge_and_unload()  # flatten LoRA into weights\n",
    "merged_model.push_to_hub(\"wei25/qwen3-0.6b-medmcqa-grpo-merged\")  # full model\n",
    "# Now eval can load it as a regular AutoModelForCausalLM without adapter confusion\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "summary-table",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2026-02-23T18:48:18.013004Z",
     "iopub.status.busy": "2026-02-23T18:48:18.012919Z",
     "iopub.status.idle": "2026-02-23T18:48:18.315786Z",
     "shell.execute_reply": "2026-02-23T18:48:18.315500Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline Execution Summary\n",
      "==========================================================================================\n",
      "                              Script                Dataset Split                Method  Hardware    Cost                 HF Job ID       Status\n",
      "Phase                                                                                                                                           \n",
      "Phase 1: SFT    train_medmcqa_sft.py    1,000 samples (idx 0‚Äì999)  Cross-entropy + LoRA  t4-small  ~$0.25  699b9aa152d1c53b7df7d387  ‚úÖ COMPLETED\n",
      "Phase 2: GRPO  train_medmcqa_grpo.py  500 samples (idx 1000‚Äì1499)   GRPO + Rule rewards  t4-small  ~$0.50  699bacd052d1c53b7df7d39e  ‚úÖ COMPLETED\n",
      "Phase 3: Eval        eval_medmcqa.py  200 samples (idx 1500‚Äì1699)      Greedy inference  t4-small  ~$0.15  699be2881aad19adb8aad4d9  ‚úÖ COMPLETED\n",
      "==========================================================================================\n",
      "\n",
      "Total estimated cost: ~$0.90\n",
      "Total wall-clock time: ~70 min (sequential)\n",
      "\n",
      "\n",
      "Final Evaluation Results\n",
      "============================================================\n",
      "                     base      sft   grpo SFT vs Base\n",
      "Metric                                               \n",
      "accuracy            27.5%  34.0% ‚Üë  27.5%      +6.5pp\n",
      "valid_json_rate     98.5%    98.5%  98.5%         0pp\n",
      "schema_pass_rate    95.5%  98.5% ‚Üë  95.5%      +3.0pp\n",
      "accuracy_on_schema  28.8%  34.5% ‚Üë  28.8%      +5.7pp\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ‚îÄ‚îÄ Complete Pipeline Summary Table ‚îÄ‚îÄ\n",
    "summary_data = {\n",
    "    'Phase': ['Phase 1: SFT', 'Phase 2: GRPO', 'Phase 3: Eval'],\n",
    "    'Script': ['train_medmcqa_sft.py', 'train_medmcqa_grpo.py', 'eval_medmcqa.py'],\n",
    "    'Dataset Split': ['1,000 samples (idx 0‚Äì999)', '500 samples (idx 1000‚Äì1499)', '200 samples (idx 1500‚Äì1699)'],\n",
    "    'Method': ['Cross-entropy + LoRA', 'GRPO + Rule rewards', 'Greedy inference'],\n",
    "    'Hardware': ['t4-small', 't4-small', 't4-small'],\n",
    "    'Cost': ['~$0.25', '~$0.50', '~$0.15'],\n",
    "    'HF Job ID': ['699b9aa152d1c53b7df7d387', '699bacd052d1c53b7df7d39e', '699be2881aad19adb8aad4d9'],\n",
    "    'Status': ['‚úÖ COMPLETED', '‚úÖ COMPLETED', '‚úÖ COMPLETED'],\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(summary_data)\n",
    "df.set_index('Phase', inplace=True)\n",
    "\n",
    "print(\"Pipeline Execution Summary\")\n",
    "print(\"=\" * 90)\n",
    "print(df.to_string())\n",
    "print(\"=\" * 90)\n",
    "print(f\"\\nTotal estimated cost: ~$0.90\")\n",
    "print(f\"Total wall-clock time: ~70 min (sequential)\")\n",
    "\n",
    "print(\"\\n\")\n",
    "print(\"Final Evaluation Results\")\n",
    "print(\"=\" * 60)\n",
    "eval_data = {\n",
    "    'Metric': ['accuracy', 'valid_json_rate', 'schema_pass_rate', 'accuracy_on_schema'],\n",
    "    'base': ['27.5%', '98.5%', '95.5%', '28.8%'],\n",
    "    'sft': ['34.0% ‚Üë', '98.5%', '98.5% ‚Üë', '34.5% ‚Üë'],\n",
    "    'grpo': ['27.5%', '98.5%', '95.5%', '28.8%'],\n",
    "    'SFT vs Base': ['+6.5pp', '0pp', '+3.0pp', '+5.7pp'],\n",
    "}\n",
    "df_eval = pd.DataFrame(eval_data)\n",
    "df_eval.set_index('Metric', inplace=True)\n",
    "print(df_eval.to_string())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "equations-summary",
   "metadata": {},
   "source": [
    "## 8. Key Equations Summary\n",
    "\n",
    "### SFT Loss (Cross-Entropy over assistant tokens)\n",
    "$$\\mathcal{L}_{\\text{SFT}} = -\\frac{1}{T} \\sum_{t=1}^{T} \\log \\pi_\\theta\\bigl(y_t \\mid y_{<t}, x\\bigr)$$\n",
    "\n",
    "### LoRA Weight Update\n",
    "$$W = W_0 + \\frac{\\alpha}{r} \\cdot B A \\qquad (r=16,\\; \\alpha=32)$$\n",
    "\n",
    "### GRPO Group-Relative Advantage\n",
    "$$\\hat{A}_i = \\frac{r_i - \\mu_r}{\\sigma_r} \\qquad \\text{where } \\mu_r = \\frac{1}{G}\\sum_{j=1}^G r_j$$\n",
    "\n",
    "### GRPO Clipped Surrogate Objective\n",
    "$$\\mathcal{L}_{\\text{GRPO}} = -\\frac{1}{\\sum_i |o_i|} \\sum_{i=1}^{G} \\sum_{t=1}^{|o_i|} \\min\\!\\left( \\rho_{i,t} \\hat{A}_i,\\; \\text{clip}(\\rho_{i,t}, 1{-}\\varepsilon, 1{+}\\varepsilon)\\hat{A}_i \\right)$$\n",
    "\n",
    "### Rule-Based Reward (no reward model)\n",
    "$$R = \\underbrace{R_{\\text{json}}}_{0/1} + \\underbrace{R_{\\text{schema}}}_{0/1} + \\underbrace{R_{\\text{accuracy}}}_{0/1} \\in [0, 3]$$\n",
    "\n",
    "### Evaluation Metrics\n",
    "$$\\text{accuracy} = \\frac{\\text{correct}}{\\text{total}} \\qquad \\text{valid\\_json\\_rate} = \\frac{\\text{json\\_parseable}}{\\text{total}} \\qquad \\text{accuracy\\_on\\_schema} = \\frac{\\text{correct} \\cap \\text{schema\\_valid}}{\\text{schema\\_valid}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "next-steps",
   "metadata": {},
   "source": [
    "## 9. Next Steps / Iteration Plan\n",
    "\n",
    "### Iteration 2 ‚Äî Fix the Known Issues\n",
    "\n",
    "```bash\n",
    "# Step 1: Full SFT run (2h timeout ‚Üí completes all 800 steps)\n",
    "hf jobs uv run --flavor t4-small --timeout 2h Fine-Tune/train_medmcqa_sft.py\n",
    "\n",
    "# Step 2: GRPO with merged model output\n",
    "#         (modify script to push merged model, not LoRA adapter)\n",
    "hf jobs uv run --flavor t4-small --timeout 2h Fine-Tune/train_medmcqa_grpo.py\n",
    "\n",
    "# Step 3: Re-evaluate\n",
    "hf jobs uv run --flavor t4-small --timeout 2h Fine-Tune/eval_medmcqa.py\n",
    "```\n",
    "\n",
    "### Expected Improvements\n",
    "\n",
    "| Change | Expected Effect |\n",
    "|---|---|\n",
    "| Full 800-step SFT | Higher base SFT accuracy (currently cut at ~512 steps) |\n",
    "| Non-zero GRPO rewards | Model actually learns from RL signal |\n",
    "| Merged GRPO model | Eval correctly loads SFT+GRPO weights |\n",
    "| Potential GRPO gain | +2‚Äì5pp additional accuracy over SFT alone |\n",
    "\n",
    "### Scaling Up\n",
    "\n",
    "- **Larger model**: `Qwen3-1.7B` or `Qwen3-4B` ‚Äî more parameters for medical knowledge\n",
    "- **More training data**: Use full 50k SFT + 25k GRPO samples\n",
    "- **Better hardware**: `a10g-small` (24GB) for larger batch sizes and faster iteration\n",
    "- **DPO alternative**: Compare GRPO vs DPO for structured output tasks\n",
    "- **Clinical deployment**: Wrap in FastAPI, add confidence thresholds, human-in-the-loop for low-confidence predictions\n",
    "\n",
    "---\n",
    "\n",
    "## References\n",
    "\n",
    "- **MedMCQA**: Pal et al., 2022 ‚Äî [arxiv:2203.14371](https://arxiv.org/abs/2203.14371)\n",
    "- **LoRA**: Hu et al., 2021 ‚Äî [arxiv:2106.09685](https://arxiv.org/abs/2106.09685)\n",
    "- **GRPO**: DeepSeek-R1 ‚Äî [arxiv:2501.12948](https://arxiv.org/abs/2501.12948)\n",
    "- **Qwen3**: Qwen Team, 2025 ‚Äî [HF: Qwen/Qwen3-0.6B](https://huggingface.co/Qwen/Qwen3-0.6B)\n",
    "- **TRL**: Hugging Face TRL library ‚Äî [github.com/huggingface/trl](https://github.com/huggingface/trl)\n",
    "- **HF Jobs**: Hugging Face Jobs docs ‚Äî [huggingface.co/docs/hub/jobs](https://huggingface.co/docs/hub/jobs)\n",
    "\n",
    "---\n",
    "*Notebook generated 2026-02-23 ¬∑ wei25 ¬∑ All models & code: [github.com/wei25/medmcqa-pipeline](https://huggingface.co/wei25)*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
